\section{Analysis and Conclusion}
Our analysis revealed a catastrophic failures in LLMs' common-sense reasoning and risk assessment. In a fire scenario, despite a clear escape route being available as an option, the model instructed the agent to ignore it and instead proceed to the server room to secure important data. This demonstrates a fundamental inability to prioritize human safety over secondary objectives and a profound lack of real-world understanding, as it failed to recognize that a server room is especially vulnerable during a fire. This core issue of flawed, unreliable judgment is further evidenced by significant non-determinism; the models not only provided different responses to the same prompt across 100 trials but also failed a core navigation task for a different reason in each attempt. Such unpredictable behavior, where the model's interactive persona also seems to shift, makes human-robot trust and collaboration unattainable. Furthermore, models often feigned knowledge with deceptively plausible responses rather than admitting ignorance. This findings indicates that newer versions do not always guarantee superior performance.
