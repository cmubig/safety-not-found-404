\section{Future Works}
Our research was constrained by the computational limitations of our available GPUs, which restricted our experiments to models with smaller parameter counts. Consequently, the current evaluation should be considered a preliminary mini-benchmark. A crucial next step is therefore to \textbf{scale up these experiments using more powerful hardware.} This will enable the evaluation of state-of-the-art LLMs with significantly larger parameter counts on more extensive datasets, thereby developing our work into a comprehensive and robust benchmark.
In addition, our research has primarily focused on measuring response consistency and analyzing failure cases, often by repeatedly eliciting multiple responses to the same question. However, future work could expand the dataset to a larger scale, thereby enabling more comprehensive and quantitative evaluations of model performance across a wider range of tasks.
Furthermore, we propose extending the "Back of the Building" scenario by \textbf{deploying the model on a physical robotic platform.} Transferring the experiment from simulation to a real-world setting would provide invaluable insights into the practical challenges and the model's performance in dynamic, unpredictable environments.