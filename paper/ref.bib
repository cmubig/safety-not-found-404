%%%%%%%%%%%%%%정빈 레퍼런스 (아래)%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{SpatialVLM,
  title     = {SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities},
  author    = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2024},
  pages     = {14455--14465}
}

@inproceedings{SpatialRGPT,
  title     = {SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models},
  author    = {Cheng, An-Chieh and Yin, Hongxu and Fu, Yang and Guo, Qiushan and Yang, Ruihan and Kautz, Jan and Wang, Xiaolong and Liu, Sifei},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2024}
}

@misc{thinking-in-space,
  title         = {Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces},
  author        = {Yang, Jihan and Yang, Shusheng and Gupta, Anjali W. and Han, Rilyn and Fei-Fei, Li and Xie, Saining},
  year          = {2025},
  eprint        = {2412.14171},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV},
  url           = {https://arxiv.org/abs/2412.14171}
}

@misc{geospatial-benchmark,
      title={Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study}, 
      author={Liuchang Xu and Shuo Zhao and Qingming Lin and Luyao Chen and Qianqian Luo and Sensen Wu and Xinyue Ye and Hailin Feng and Zhenhong Du},
      year={2025},
      eprint={2408.14438},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2408.14438}
}



@article{spatialcognition,
  title={Evaluating and enhancing spatial cognition abilities of large language models},
  author={Yang, Anran and Fu, Cheng and Jia, Qingren and Dong, Weihua and Ma, Mengyu and Chen, Hao and Yang, Fei and Wu, Hui},
  journal={International Journal of Geographical Information Science},
  pages={1--36},
  year={2025},
  publisher={Taylor \& Francis}
}


@article{plugh,
  title   = {PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models},
  author  = {Li, Xinyu and Chen, Hao and Sun, Yifan and others},
  journal = {arXiv preprint arXiv:2408.04648},
  year    = {2024},
  url     = {https://arxiv.org/abs/2408.04648}
}

@inproceedings{vot,
  title     = {Mind’s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning},
  author    = {Zhao, Xinyi and Xu, Tianyu and Chen, Yuchen and others},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2024}
}

@inproceedings{spatialprompt,
  title     = {Q-Spatial Bench: Benchmarking and Prompting for Spatial Reasoning},
  author    = {Liao, Andrew and others},
  booktitle = {Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2024},
  url       = {https://andrewliao11.github.io/spatial_prompt/}
}

@article{forest,
  title={FoREST: Frame of Reference Evaluation in Spatial Reasoning Tasks},
  author={Premsri, Tanawan and Kordjamshidi, Parisa},
  journal={arXiv preprint arXiv:2502.17775},
  year={2025}
}

@article{planqa,
  title   = {PlanQA: A Benchmark for Spatial Reasoning in Large Language Models},
  author  = {Zhang, Yifan and Liu, Chen and Zhao, Yizhou and others},
  journal = {arXiv preprint arXiv:2507.07644},
  year    = {2025},
  url     = {https://arxiv.org/abs/2507.07644}
}

@inproceedings{spatialeval,
  title     = {Is a Picture Worth a Thousand Words? Delving Into Spatial Reasoning for Vision-Language Models},
  author    = {Wang, Jiayu and Liu, Shuzheng and Chen, Xin and others},
  booktitle = {NeurIPS 2024 Workshop on Multimodal Reasoning},
  year      = {2024},
  url       = {https://openreview.net/forum?id=cvaSru8LeO}
}

@inproceedings{embspatial-bench,
  title     = {EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models},
  author    = {Du, Mengfei and Wu, Binhao and Li, Zejun and Huang, Xuanjing and Wei, Zhongyu},
  booktitle = {Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  year      = {2024},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics},
  pages     = {346--355},
  doi       = {10.18653/v1/2024.acl-short.33},
  url       = {https://aclanthology.org/2024.acl-short.33/}
}

@misc{spinbench,
  title   = {SpinBench: Perspective and Rotation as a Lens on Spatial Reasoning in VLMs},
  author  = {Anonymous},
  year    = {2025},
  eprint  = {2509.25390},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CV}
}

@misc{selfprompting,
      title={Self-Prompting Large Language Models for Zero-Shot Open-Domain QA}, 
      author={Junlong Li and Jinyuan Wang and Zhuosheng Zhang and Hai Zhao},
      year={2024},
      eprint={2212.08635},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2212.08635}, 
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%% 주아가 단 레퍼런스 (아래)%%%%%%%%%%%%%%%%%%%%
% 선정 이유: SayCan 논문으로, LLM을 로봇의 '두뇌'로 사용한다는 개념을 확립한 선구적인 연구입니다. LLM이 생성한 계획을 로봇이 실제로 수행할 수 있는 행동(affordance)에 기반하여 필터링하는 방식을 제안했습니다.
@inproceedings{brohan2023can,
  title={Do as i can, not as i say: Grounding language in robotic affordances},
  author={Brohan, Anthony and Chebotar, Yevgen and Finn, Chelsea and Hausman, Karol and Herzog, Alexander and Ho, Daniel and Ibarz, Julian and Irpan, Alex and Jang, Eric and Julian, Ryan and others},
  booktitle={Conference on robot learning},
  pages={287--318},
  year={2023},
  organization={PMLR}
}

% LLM이 별도의 학습 없이도 상식적인 지식을 활용하여 복잡한 로봇 작업 계획을 생성할 수 있음을 보여준 초기 연구 중 하나입니다. LLM을 '제로샷 플래너'로 활용하는 개념을 제시했습니다.
@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International conference on machine learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}

% LLM이 자연어 명령을 파이썬 코드 형태의 로봇 정책으로 변환하는 "Code as Policies" 접근법을 제안했습니다. 이는 LLM의 코드 생성 능력을 로봇 제어에 직접적으로 활용한 혁신적인 아이디어입니다.
@article{liang2022code,
  title={Code as policies: Language model programs for embodied control},
  author={Liang, Jacky and Huang, Wenlong and Xia, Fei and Xu, Peng and Hausman, Karol and Ichter, Brian and Florence, Pete and Zeng, Andy},
  journal={arXiv preprint arXiv:2209.07753},
  year={2022}
}

% 사용자의 지시뿐만 아니라 현재 환경에 대한 정보를 함께 프롬프트로 제공하여, 상황에 맞는(situated) 로봇 계획을 생성하는 ProgPrompt 방법론을 제안했습니다. LLM의 상황 인지 능력을 강조합니다.
@article{singh2022progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  journal={arXiv preprint arXiv:2209.11302},
  year={2022}
}

% 선정 이유: LLM을 로보틱스에 적용하는 연구 동향을 포괄적으로 정리한 서베이 논문입니다. 이 분야의 전체적인 흐름과 주요 개념을 이해하고 인용하기에 매우 적합합니다.
@article{wang2025large,
  title={Large language models for robotics: Opportunities, challenges, and perspectives},
  author={Wang, Jiaqi and Shi, Enze and Hu, Huawen and Ma, Chong and Liu, Yiheng and Wang, Xuhui and Yao, Yincheng and Liu, Xuan and Ge, Bao and Zhang, Shu},
  journal={Journal of Automation and Intelligence},
  volume={4},
  number={1},
  pages={52--64},
  year={2025},
  publisher={Elsevier}
}

% 선정 이유: RT-2는 웹 스케일의 시각-언어 데이터로 사전 학습된 모델이 로봇 제어에 직접적으로 전이될 수 있음을 보여준 획기적인 연구입니다. 멀티모달 모델이 어떻게 웹의 방대한 지식을 로봇 행동 생성에 활용하는지 보여주는 핵심 사례입니다.
@inproceedings{zitkovich2023rt,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Zitkovich, Brianna and Yu, Tianhe and Xu, Sichun and Xu, Peng and Xiao, Ted and Xia, Fei and Wu, Jialin and Wohlhart, Paul and Welker, Stefan and Wahid, Ayzaan and others},
  booktitle={Conference on Robot Learning},
  pages={2165--2183},
  year={2023},
  organization={PMLR}
}

% 선정 이유: 다양한 로봇 팔에서 수집된 데이터를 학습하여 새로운 로봇과 태스크에 빠르게 적응하는 RoboCat 에이전트를 제안했습니다. 여러 종류의 멀티모달 데이터를 활용하여 일반화 성능을 높인 좋은 예시입니다.
@article{bousmalis2023robocat,
  title={Robocat: A self-improving generalist agent for robotic manipulation},
  author={Bousmalis, Konstantinos and Vezzani, Giulia and Rao, Dushyant and Devin, Coline and Lee, Alex X and Bauz{\'a}, Maria and Davchev, Todor and Zhou, Yuxiang and Gupta, Agrim and Raju, Akhil and others},
  journal={arXiv preprint arXiv:2306.11706},
  year={2023}
}

% 선정 이유: 언어, 이미지, 촉각 등 다양한 감각 입력을 하나의 트랜스포머 아키텍처(Perceiver)로 처리하여 로봇 행동을 생성하는 방법을 제시했습니다. 멀티모달리티를 통합하는 아키텍처 관점에서 의미 있는 연구입니다.
@inproceedings{shridhar2023perceiver,
  title={Perceiver-actor: A multi-task transformer for robotic manipulation},
  author={Shridhar, Mohit and Manuelli, Lucas and Fox, Dieter},
  booktitle={Conference on Robot Learning},
  pages={785--799},
  year={2023},
  organization={PMLR}
}

%%%%% ref1
% AI 안전성의 핵심 문제(robustness, side effects, reward hacking 등)를 제시한 고전적 연구
@article{amodei2016concrete,
  title={Concrete problems in AI safety},
  author={Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  journal={arXiv preprint arXiv:1606.06565},
  year={2016}
}

% 시스템 안전 공학의 근본 원리 — “한 번의 오류도 치명적” 개념을 기술적으로 뒷받침.
@book{leveson2016engineering,
  title={Engineering a safer world: Systems thinking applied to safety},
  author={Leveson, Nancy G},
  year={2016},
  publisher={The MIT Press}
}

% ML 모델을 안전 관점에서 다루는 방법론적 프레임워크 제시.
@inproceedings{varshney2016engineering,
  title={Engineering safety in machine learning},
  author={Varshney, Kush R},
  booktitle={2016 Information Theory and Applications Workshop (ITA)},
  pages={1--5},
  year={2016},
  organization={IEEE}
}

% 윤리적 정렬 실패가 사회적으로 치명적 결과를 초래할 수 있음을 경고.
@article{gabriel2020artificial,
  title={Artificial intelligence, values, and alignment},
  author={Gabriel, Iason},
  journal={Minds and machines},
  volume={30},
  number={3},
  pages={411--437},
  year={2020},
  publisher={Springer}
}

% 실제 의료 시스템 사고를 통해 safety-critical error의 심각성 입증.
@article{leveson1993investigation,
  title={An investigation of the Therac-25 accidents},
  author={Leveson, Nancy G and Turner, Clark S},
  journal={Computer},
  volume={26},
  number={7},
  pages={18--41},
  year={1993},
  publisher={IEEE}
}


%%%%%%%% ref2

% LLMs를 로보틱스에 적용할 때의 잠재적 리스크와 도전 과제를 종합
@article{zeng2023large,
  title={Large language models for robotics: A survey},
  author={Zeng, Fanlong and Gan, Wensheng and Wang, Yongheng and Liu, Ning and Yu, Philip S},
  journal={arXiv preprint arXiv:2311.07226},
  year={2023}
}

% LLM의 가치 불일치(alignment gap)가 물리적 행동 단계에서 위험화될 수 있음을 논의.
@article{hendrycks2020aligning,
  title={Aligning ai with shared human values},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2008.02275},
  year={2020}
}

% 단순한 정답률 외에 실제 위험도 평가의 필요성 제시.
@article{liang2022holistic,
  title={Holistic evaluation of language models},
  author={Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and others},
  journal={arXiv preprint arXiv:2211.09110},
  year={2022}
}


%%%%% ref3

% 해석 가능성 부족이 신뢰 문제로 이어짐을 강조.
@article{doshi2017towards,
  title={Towards a rigorous science of interpretable machine learning},
  author={Doshi-Velez, Finale and Kim, Been},
  journal={arXiv preprint arXiv:1702.08608},
  year={2017}
}

% 성능이 높아도 특정 조건에서 취약한 사례 다수 제시.
@inproceedings{hendrycks2021many,
  title={The many faces of robustness: A critical analysis of out-of-distribution generalization},
  author={Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and others},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8340--8349},
  year={2021}
}

% 통계적 정확도와 실제 신뢰성 간의 괴리를 비판.
@article{marcus2022deep,
  title={Deep learning is hitting a wall},
  author={Marcus, Gary},
  journal={Nautilus},
  volume={10},
  pages={2022},
  year={2022}
}

% “고성능 ≠ 안전성”이라는 사회·윤리적 위험 경고.
@inproceedings{bender2021dangers,
  title={On the dangers of stochastic parrots: Can language models be too big?},
  author={Bender, Emily M and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={610--623},
  year={2021}
}

%%%%%%%%%%%%%%%% ref4

% 모델이 자신감 있게 거짓 정보를 생성하는 경향을 정량화함.
@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

% LLM의 자기 확신(self-assessment) 한계 및 과신 문제를 실험적으로 검증.
@article{kadavath2022language,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}

% LLM의 추론 과정이 표면적으로는 논리적이지만 실제 근거와 불일치할 수 있음을 지적.
@article{turpin2023language,
  title={Language models don't always say what they think: Unfaithful explanations in chain-of-thought prompting},
  author={Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={74952--74965},
  year={2023}
}

% 다양한 조건에서 LLM이 얼마나 자주 맥락을 무시하고 허위 추론을 하는지 평가.
@article{bang2023multitask,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}

% early hallucination 연구로, "grounded reasoning failure" 개념의 근거 제공.
@article{maynez2020faithfulness,
  title={On faithfulness and factuality in abstractive summarization},
  author={Maynez, Joshua and Narayan, Shashi and Bohnet, Bernd and McDonald, Ryan},
  journal={arXiv preprint arXiv:2005.00661},
  year={2020}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%재윤 레퍼런스 (아래)%%%%%%%%%%%%%%%%%%%%%%%%%

%선정이유 : vln 스탠다드 metric
@inproceedings{anderson2018vision,
  title={Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments},
  author={Anderson, Peter and Wu, Qi and Teney, Damien and Bruce, Jake and Johnson, Mark and S{\"u}nderhauf, Niko and Reid, Ian and Gould, Stephen and Van Den Hengel, Anton},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3674--3683},
  year={2018}
}

%선정이유 : vln 스탠다드 metric 2
@article{ilharco2019general,
  title={General evaluation for instruction conditioned navigation using dynamic time warping},
  author={Ilharco, Gabriel and Jain, Vihan and Ku, Alexander and Ie, Eugene and Baldridge, Jason},
  journal={arXiv preprint arXiv:1907.05446},
  year={2019}
}

%선정이유 : vln 논문
@article{pan2023langnav,
  title={Langnav: Language as a perceptual representation for navigation},
  author={Pan, Bowen and Panda, Rameswar and Jin, SouYoung and Feris, Rogerio and Oliva, Aude and Isola, Phillip and Kim, Yoon},
  journal={arXiv preprint arXiv:2310.07889},
  year={2023}
}
%선정이유 : vln 논문
@inproceedings{krantz2020beyond,
  title={Beyond the nav-graph: Vision-and-language navigation in continuous environments},
  author={Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
  booktitle={European Conference on Computer Vision},
  pages={104--120},
  year={2020},
  organization={Springer}
}
%선정이유 : vln 논문
@article{ku2020room,
  title={Room-across-room: Multilingual vision-and-language navigation with dense spatiotemporal grounding},
  author={Ku, Alexander and Anderson, Peter and Patel, Roma and Ie, Eugene and Baldridge, Jason},
  journal={arXiv preprint arXiv:2010.07954},
  year={2020}
}
%선정이유 : vln 논문
@article{kuang2024openfmnav,
  title={Openfmnav: Towards open-set zero-shot object navigation via vision-language foundation models},
  author={Kuang, Yuxuan and Lin, Hai and Jiang, Meng},
  journal={arXiv preprint arXiv:2402.10670},
  year={2024}
}
%선정이유 : vln 논문
@article{zhang2024navid,
  title={Navid: Video-based vlm plans the next step for vision-and-language navigation},
  author={Zhang, Jiazhao and Wang, Kunyu and Xu, Rongtao and Zhou, Gengze and Hong, Yicong and Fang, Xiaomeng and Wu, Qi and Zhang, Zhizheng and Wang, He},
  journal={arXiv preprint arXiv:2402.15852},
  year={2024}
}

%선정이유 : 아부현상. 이거 밑으로는 카톡 아직 확인 안함! (새벽5시에 작성해서 알림 이슈. 해뜨면 확인 예정)
@article{sharma2023towards,
  title={Towards understanding sycophancy in language models},
  author={Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R and Cheng, Newton and Durmus, Esin and Hatfield-Dodds, Zac and Johnston, Scott R and others},
  journal={arXiv preprint arXiv:2310.13548},
  year={2023}
}

%선정이유 : 방향구분못한다 근거자료 
@article{hoehing2023s,
  title={What's left can't be right--The remaining positional incompetence of contrastive vision-language models},
  author={Hoehing, Nils and Rushe, Ellen and Ventresque, Anthony},
  journal={arXiv preprint arXiv:2311.11477},
  year={2023}
}

%선정이유: 동일
@article{kamath2023s,
  title={What's" up" with vision-language models? investigating their struggle with spatial reasoning},
  author={Kamath, Amita and Hessel, Jack and Chang, Kai-Wei},
  journal={arXiv preprint arXiv:2310.19785},
  year={2023}
}

%선정이유: 동일. 다만 task가 겹치니 이걸 알고있음에도 우리가 따로 실험을 한 이유를 명시해야할듯 
@article{kong2025lrr,
  title={Lrr-bench: Left, right or rotate? vision-language models still struggle with spatial understanding tasks},
  author={Kong, Fei and Duan, Jinhao and Xu, Kaidi and Guo, Zhenhua and Zhu, Xiaofeng and Shi, Xiaoshuang},
  journal={arXiv preprint arXiv:2507.20174},
  year={2025}
}

%선정이유 : 아부현상
@inproceedings{malmqvist2025sycophancy,
  title={Sycophancy in large language models: Causes and mitigations},
  author={Malmqvist, Lars},
  booktitle={Intelligent Computing-Proceedings of the Computing Conference},
  pages={61--74},
  year={2025},
  organization={Springer}
}

%선정이유 : 멀티모달 환각 논문
@article{bai2024hallucination,
  title={Hallucination of multimodal large language models: A survey},
  author={Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2404.18930},
  year={2024}
}

%선정이유 : 환각 논문
@article{huang2025survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  volume={43},
  number={2},
  pages={1--55},
  year={2025},
  publisher={ACM New York, NY}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%