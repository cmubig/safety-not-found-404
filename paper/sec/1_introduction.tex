\section{Introduction}
\label{sec:intro}
In safety-critical domains, even a single error can result in fatality ~\cite{amodei2016concrete, leveson2016engineering, varshney2016engineering, gabriel2020artificial, leveson1993investigation}. As LLMs become the core of autonomous and assistive systems~\cite{brohan2023can, huang2022language, liang2022code, singh2022progprompt, wang2025large, zitkovich2023rt, bousmalis2023robocat, shridhar2023perceiver}, their statistical success can mask catastrophic physical risks~\cite{zeng2023large, hendrycks2020aligning, liang2022holistic}. A model with 99\% accuracy may still fail once in a hundred trials, an unacceptable margin when human lives are involved~\cite{varshney2016engineering, doshi2017towards, hendrycks2021many, marcus2022deep, bender2021dangers}. For instance, we tested the latest models including Gemini-2.5 Flash and GPT-5 in a simulated fire evacuation scenario as illustrated in~Fig~\ref{fig_abstract}. Despite clear instructions identifying an emergency exit, the models advised moving toward a server room, disregarding the contextual cues in the prompt. This failure exemplifies how LLMs can produce confident yet dangerously ungrounded reasoning, revealing their lack of contextual judgment in high-stakes decision-making~\cite{huang2025survey, lin2021truthfulqa, kadavath2022language, turpin2023language, bang2023multitask, maynez2020faithfulness}.

Motivated by these findings, we examine the safety reliability of current LLMs and VLMs through seven diagnostic tasks spanning basic to safety-critical spatial reasoning. These include complete-information map tasks, incomplete scenarios requiring inference, and natural-language-based SOSR tasks. This framework enables both average- and worst-case analyses of spatial reasoning and safety behavior. Our results and analyses underscore a critical message: modern LLMs, despite their impressive benchmarks, cannot yet be trusted as autonomous decision-makers in real-world safety-critical settings. The main contributions of this work are as follows:
\begin{enumerate}
  \item We empirically demonstrate that even the latest LLMs with improved performance do not guarantee safety in practice.
  \item We systematize concrete failure cases to expose the hazards that can arise in safety-critical environments.
  \item We highlight how absolute reliance on AI in safety-critical domains can itself become a new source of risk.
  \item We demonstrate why metrics like 99\% accuracy are insufficient and dangerously misleading for safety evaluation.
\end{enumerate}
